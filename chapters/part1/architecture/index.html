<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chapter 3: Understanding AudioCraft Architecture</title>
  <meta name="description" content="Chapter 3: Understanding AudioCraft Architecture">
  <link rel="canonical" href="https://scttfrdmn.github.io/practical-meta-audiocraft/chapters/part1/architecture/">
  
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      color: #333;
      margin: 0;
      padding: 0;
    }
    .wrapper {
      max-width: 100%;
    }
    .container {
      max-width: 960px;
      margin: 0 auto;
      padding: 0 20px;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 2rem;
      margin-bottom: 1rem;
    }
    h1 { color: #1a73e8; }
    h2 { color: #34a853; }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .site-header {
      border-bottom: 1px solid #eee;
      padding: 15px 0;
      margin-bottom: 30px;
    }
    .site-title {
      margin: 0;
      font-size: 1.8rem;
    }
    .site-title a {
      color: #1a73e8;
      text-decoration: none;
    }
    .site-nav {
      float: right;
      margin-top: 10px;
    }
    .site-nav .page-link {
      margin-left: 20px;
    }
    .nav-trigger {
      display: none;
    }
    .menu-icon {
      display: none;
    }
    .page-content {
      padding: 20px 0;
    }
    .site-footer {
      border-top: 1px solid #eee;
      padding: 30px 0;
      margin-top: 60px;
    }
    .footer-col-wrapper {
      display: flex;
      flex-wrap: wrap;
    }
    .footer-col {
      flex: 1;
      min-width: 200px;
      padding-right: 20px;
    }
    code {
      background-color: #f5f5f5;
      padding: 2px 5px;
      border-radius: 3px;
    }
    pre {
      background-color: #f5f5f5;
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
    }
    .highlight {
      background-color: #f5f5f5;
      border-radius: 5px;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin-bottom: 20px;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 8px 12px;
    }
    th {
      background-color: #f5f5f5;
      text-align: left;
    }
    button, .button {
      display: inline-block;
      background-color: #1a73e8;
      color: white !important;
      padding: 10px 20px;
      border-radius: 4px;
      font-weight: bold;
      margin-top: 20px;
      border: none;
      cursor: pointer;
      text-decoration: none !important;
    }
    button:hover, .button:hover {
      background-color: #0d65d9;
    }
    .feature {
      background-color: #f5f5f5;
      border-radius: 8px;
      padding: 20px;
      border-left: 4px solid #1a73e8;
      margin-bottom: 20px;
    }
    
    @media screen and (max-width: 600px) {
      .site-nav {
        position: absolute;
        top: 70px;
        right: 20px;
        background-color: white;
        border: 1px solid #ddd;
        border-radius: 5px;
        text-align: right;
        padding: 0;
        z-index: 1;
      }
      .site-nav .page-link {
        display: block;
        padding: 10px;
        margin: 0;
      }
      .site-nav .menu-icon {
        display: block;
        float: right;
        width: 36px;
        height: 26px;
        line-height: 0;
        padding-top: 10px;
        text-align: center;
      }
      .site-nav .trigger {
        clear: both;
        display: none;
      }
      .site-nav input:checked ~ .trigger {
        display: block;
        padding-bottom: 5px;
      }
      .footer-col-wrapper {
        flex-direction: column;
      }
      .footer-col {
        margin-bottom: 20px;
      }
    }
  </style>
</head><body>
    <div class="wrapper">
      <header class="site-header">
        <div class="container">
          <h1 class="site-title"><a href="/practical-meta-audiocraft/">Practical Meta AudioCraft</a></h1>
          <nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
              <span class="menu-icon">
                <svg viewBox="0 0 18 15" width="18px" height="15px">
                  <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484c0-0.82,0.665-1.485,1.484-1.485 h15.032C17.335,0,18,0.665,18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516 c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,6.031,18,6.696,18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516c0-0.82,0.665-1.483,1.484-1.483h15.032 C17.335,12.031,18,12.696,18,13.516z"/>
                </svg>
              </span>
            </label>

            <div class="trigger">
              <a class="page-link" href="/practical-meta-audiocraft/">Home</a>
              <a class="page-link" href="/practical-meta-audiocraft/chapters/part1/introduction/">Getting Started</a>
              <a class="page-link" href="/practical-meta-audiocraft/chapters/part2/basic-music/">MusicGen</a>
              <a class="page-link" href="/practical-meta-audiocraft/chapters/part3/introduction-to-audiogen/">AudioGen</a>
              <a class="page-link" href="/practical-meta-audiocraft/tutorials/getting-started/">Tutorials</a>
            </div>
          </nav>
        </div>
      </header>

      <main class="page-content" aria-label="Content">
        <div class="container">
          <div class="chapter">
  <div class="chapter-header">
    <div class="chapter-metadata">
      <span class="difficulty intermediate">Intermediate</span>
      <span class="estimated-time">3 hours</span>
    </div>
    <h1>Chapter 3: Understanding AudioCraft Architecture</h1>
  </div>
  
  
  <div class="scenario-quote">
    <blockquote>
      <p>"I've been using AudioCraft to generate audio for my projects, but I feel like I'm just scratching the surface. I want to understand how it actually works under the hood so I can better control the output and maybe even customize it for my specific needs."</p>
      <cite>— Taylor Rodriguez, Creative Technologist</cite>
    </blockquote>
  </div>
  
  
  <h1 id="chapter-3-understanding-audiocraft-architecture">Chapter 3: Understanding AudioCraft Architecture</h1>

<h2 id="the-challenge">The Challenge</h2>

<p>“Black box” AI systems can seem magical but limiting. When we don’t understand how they work, we can’t fully control them, troubleshoot effectively, or customize them for specific needs. For many creators and developers working with AudioCraft, this lack of understanding leads to a trial-and-error approach — repeatedly tweaking prompts and parameters without a clear mental model of why certain changes produce certain results.</p>

<p>This knowledge gap becomes particularly frustrating when you need precise control over the generated audio or when you encounter limitations that you don’t know how to work around. How does AudioCraft transform text into sound? Why do certain prompts work better than others? What’s happening when generation seems to get “stuck” or produces unexpected results?</p>

<p>In this chapter, we’ll demystify AudioCraft’s internal architecture. While we won’t cover every mathematical detail, you’ll gain a solid conceptual understanding of how AudioCraft’s components work together to generate audio from text, allowing you to use the framework more effectively and creatively.</p>

<h2 id="learning-objectives">Learning Objectives</h2>

<p>By the end of this chapter, you’ll be able to:</p>

<ul>
  <li>Understand the core components of AudioCraft’s architecture</li>
  <li>Explain how text prompts are processed and transformed into audio</li>
  <li>Identify the differences between MusicGen and AudioGen architectures</li>
  <li>Recognize how different generation parameters affect the model behavior</li>
  <li>Visualize the audio generation process from input to output</li>
  <li>Apply architectural knowledge to troubleshoot and optimize generation</li>
</ul>

<h2 id="core-components-overview">Core Components Overview</h2>

<p>AudioCraft consists of three primary components working together:</p>

<ol>
  <li><strong>Text Encoder</strong>: Transforms text prompts into numerical representations</li>
  <li><strong>Audio Generation Model</strong>: Creates audio in a compressed token space</li>
  <li><strong>Audio Decoder (EnCodec)</strong>: Converts compressed tokens back into waveforms</li>
</ol>

<p>Let’s visualize this high-level architecture:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text Prompt → [Text Encoder] → Text Embeddings
                                     ↓
Text Embeddings → [Generator Model] → Audio Tokens
                                          ↓
Audio Tokens → [EnCodec Decoder] → Audio Waveform
</code></pre></div></div>

<p>This modular design allows each component to specialize in a specific task, resulting in high-quality audio generation that would be difficult to achieve with a single end-to-end model.</p>

<h2 id="diving-deeper-the-text-encoder">Diving Deeper: The Text Encoder</h2>

<p>The text encoder converts natural language descriptions into numerical representations that the generator model can understand.</p>

<h3 id="how-the-text-encoder-works">How the Text Encoder Works</h3>

<ol>
  <li><strong>Tokenization</strong>: The text prompt is split into tokens (words or subwords)</li>
  <li><strong>Embedding</strong>: Each token is converted to a vector (numerical representation)</li>
  <li><strong>Contextual Processing</strong>: A transformer network processes these vectors to capture relationships between words</li>
</ol>

<p>AudioCraft uses a pre-trained text encoder (similar to those in large language models) that has already learned to represent textual meaning. This allows the model to understand complex musical and sound concepts described in natural language.</p>

<h3 id="the-role-of-text-embeddings">The Role of Text Embeddings</h3>

<p>Text embeddings contain rich semantic information about:</p>

<ul>
  <li><strong>Content</strong>: What instruments, sounds, or elements should be present</li>
  <li><strong>Style</strong>: The aesthetic qualities described (e.g., “ambient”, “upbeat”)</li>
  <li><strong>Structure</strong>: Temporal organization suggested by the prompt</li>
  <li><strong>Relationships</strong>: How different elements relate to each other</li>
</ul>

<p>These embeddings guide the generation process, acting as a blueprint for the audio to be created.</p>

<h2 id="the-generation-models-musicgen-and-audiogen">The Generation Models: MusicGen and AudioGen</h2>

<p>The generation models are where text descriptions are transformed into audio representations. Both MusicGen and AudioGen share a similar core architecture but are trained on different datasets for their specialized purposes.</p>

<h3 id="transformer-architecture-basics">Transformer Architecture Basics</h3>

<p>Both models use transformer-based architectures, which have revolutionized AI across domains. Here’s a simplified explanation of how transformers work:</p>

<ol>
  <li><strong>Self-Attention</strong>: Allows the model to focus on relevant parts of the input</li>
  <li><strong>Feed-Forward Networks</strong>: Process information from attention mechanisms</li>
  <li><strong>Layer Normalization</strong>: Stabilizes the learning process</li>
  <li><strong>Residual Connections</strong>: Help information flow through deep networks</li>
</ol>

<p>Transformers excel at capturing long-range dependencies, which is crucial for generating coherent musical phrases and sound sequences.</p>

<h3 id="autoregressive-generation">Autoregressive Generation</h3>

<p>AudioCraft models generate audio autoregressively, meaning they produce one token at a time, with each new token conditioned on all previous tokens:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Start with text condition and optional seed tokens
2. Predict the next token based on text condition and all previous tokens
3. Add the predicted token to the sequence
4. Repeat steps 2-3 until the desired length is reached
</code></pre></div></div>

<p>This approach allows the model to maintain consistency throughout the generation process.</p>

<h3 id="working-with-the-compressed-audio-space">Working with the Compressed Audio Space</h3>

<p>A key innovation in AudioCraft is that it doesn’t generate raw audio waveforms directly. Instead, it works with a compressed representation:</p>

<ol>
  <li><strong>Compressed Tokens</strong>: The model generates discrete tokens that represent compressed audio</li>
  <li><strong>Lower Dimensionality</strong>: Working with ~75x fewer tokens than raw audio</li>
  <li><strong>Semantic Understanding</strong>: These tokens capture higher-level audio structures</li>
</ol>

<p>This compressed representation makes the generation task more tractable, allowing for longer and more coherent outputs.</p>

<h3 id="differences-between-musicgen-and-audiogen">Differences Between MusicGen and AudioGen</h3>

<p>While sharing architectural similarities, these models have important differences:</p>

<h4 id="musicgen">MusicGen</h4>
<ul>
  <li>Trained primarily on music datasets</li>
  <li>Optimized for musical structure and harmony</li>
  <li>Better at capturing compositional elements (melody, rhythm, harmony)</li>
  <li>Available in three sizes: small, medium, and large</li>
</ul>

<h4 id="audiogen">AudioGen</h4>
<ul>
  <li>Trained primarily on environmental and sound effect datasets</li>
  <li>Optimized for natural and mechanical sounds</li>
  <li>Better at capturing acoustic properties of real-world sounds</li>
  <li>Available in two sizes: medium and large</li>
</ul>

<p>These specializations make each model better suited for different applications, which is why AudioCraft provides both.</p>

<h2 id="the-encodec-decoder">The EnCodec Decoder</h2>

<p>After the generator model produces audio tokens, the EnCodec decoder converts these compressed representations back into raw audio waveforms that we can hear.</p>

<h3 id="neural-audio-codec">Neural Audio Codec</h3>

<p>EnCodec is a neural audio codec that:</p>

<ol>
  <li><strong>Compresses Audio</strong>: Reduces audio to a compact representation</li>
  <li><strong>Preserves Quality</strong>: Maintains high fidelity despite compression</li>
  <li><strong>Enables Efficient Generation</strong>: Makes audio generation computationally feasible</li>
</ol>

<p>During training, AudioCraft models learn to predict the compressed tokens that EnCodec creates from audio. During generation, EnCodec reverses this process, turning predicted tokens back into audio.</p>

<h3 id="the-compression-process">The Compression Process</h3>

<p>To understand EnCodec, it helps to see how audio compression works:</p>

<ol>
  <li><strong>Encoder</strong>: Raw audio → Compressed tokens
    <ul>
      <li>Used during training to create targets for the generator model</li>
    </ul>
  </li>
  <li><strong>Quantizer</strong>: Continuous values → Discrete tokens
    <ul>
      <li>Creates a finite vocabulary of possible audio states</li>
    </ul>
  </li>
  <li><strong>Decoder</strong>: Compressed tokens → Raw audio
    <ul>
      <li>Used during generation to convert model outputs to waveforms</li>
    </ul>
  </li>
</ol>

<p>This process significantly reduces the dimensionality of the audio while preserving perceptually important features.</p>

<h2 id="connecting-the-pieces-end-to-end-generation">Connecting the Pieces: End-to-End Generation</h2>

<p>Now that we’ve examined each component, let’s walk through the complete generation process:</p>

<h3 id="1-processing-the-text-prompt">1. Processing the Text Prompt</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Text prompt input
</span><span class="n">prompt</span> <span class="o">=</span> <span class="s">"An upbeat electronic track with a catchy melody"</span>

<span class="c1"># Internally, the text is tokenized and encoded
</span><span class="n">text_tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="n">text_embeddings</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span><span class="n">text_tokens</span><span class="p">)</span>
</code></pre></div></div>

<p>The text encoder converts the prompt into embeddings that capture its semantic meaning.</p>

<h3 id="2-generating-audio-tokens">2. Generating Audio Tokens</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize with text embeddings
</span><span class="n">conditioning</span> <span class="o">=</span> <span class="n">text_embeddings</span>

<span class="c1"># Generate audio tokens autoregressively
</span><span class="n">audio_tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">):</span>
    <span class="c1"># Predict next token based on previous tokens and text
</span>    <span class="n">next_token</span> <span class="o">=</span> <span class="n">generator_model</span><span class="p">(</span><span class="n">audio_tokens</span><span class="p">,</span> <span class="n">conditioning</span><span class="p">)</span>
    <span class="n">audio_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
</code></pre></div></div>

<p>The generator model produces a sequence of audio tokens based on the text condition and previously generated tokens.</p>

<h3 id="3-decoding-to-audio-waveform">3. Decoding to Audio Waveform</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert tokens to audio
</span><span class="n">waveform</span> <span class="o">=</span> <span class="n">encodec_decoder</span><span class="p">(</span><span class="n">audio_tokens</span><span class="p">)</span>

<span class="c1"># Process the waveform (normalization, etc.)
</span><span class="n">final_audio</span> <span class="o">=</span> <span class="n">post_process</span><span class="p">(</span><span class="n">waveform</span><span class="p">)</span>
</code></pre></div></div>

<p>EnCodec decodes the audio tokens into a raw audio waveform that can be played or saved.</p>

<h2 id="generation-parameters-and-their-effect">Generation Parameters and Their Effect</h2>

<p>Understanding the architecture helps explain how different parameters affect generation:</p>

<h3 id="temperature">Temperature</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>What it controls</strong>: Randomness/variability in the prediction of the next token.</p>

<p><strong>Architectural impact</strong>: Higher values (&gt;1.0) increase randomness by flattening the probability distribution of next-token predictions, while lower values (&lt;1.0) make the model more deterministic by sharpening the distribution.</p>

<p><strong>When to adjust</strong>: Increase for more creative/varied outputs; decrease for more predictable/consistent outputs.</p>

<h3 id="top-k-sampling">Top-k Sampling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">top_k</span><span class="o">=</span><span class="mi">250</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>What it controls</strong>: The number of most likely next tokens considered at each generation step.</p>

<p><strong>Architectural impact</strong>: Restricts the model to only consider the k most probable tokens, discarding unlikely options. This helps prevent the model from generating unlikely or nonsensical content.</p>

<p><strong>When to adjust</strong>: Lower values for more focused/consistent generation; higher values for more diversity.</p>

<h3 id="top-p-nucleus-sampling">Top-p (Nucleus) Sampling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>What it controls</strong>: Dynamically restricts token selection to the smallest set whose cumulative probability exceeds p.</p>

<p><strong>Architectural impact</strong>: Adapts the number of candidate tokens based on the confidence of the model’s predictions. When the model is confident, fewer options are considered; when uncertain, more options remain available.</p>

<p><strong>When to adjust</strong>: Lower values for more predictable output; higher values for more variety.</p>

<h3 id="classifier-free-guidance-cfg_coef">Classifier-Free Guidance (cfg_coef)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">cfg_coef</span><span class="o">=</span><span class="mf">3.0</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>What it controls</strong>: How closely the generation adheres to the text prompt.</p>

<p><strong>Architectural impact</strong>: Interpolates between unconditional generation (ignoring the prompt) and conditional generation (following the prompt). Higher values push generations to more closely follow the prompt.</p>

<p><strong>When to adjust</strong>: Increase for stricter adherence to the prompt; decrease for more creative freedom.</p>

<h2 id="architectural-visualization-of-the-generation-process">Architectural Visualization of the Generation Process</h2>

<p>To solidify our understanding, let’s visualize the entire process with more detail:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Text Prompt
    ↓
[Text Tokenization]
    ↓
Text Tokens
    ↓
[Text Encoder (Transformer)]
    ↓
Text Embeddings
    ↓
[Conditioning Integration]
    ↓
Initial State
    ↓
[Generation Loop]
    │
    ├── Previous Tokens + Text Condition
    │       ↓
    │   [Transformer Layers]
    │       ↓
    │   [Attention Mechanisms]
    │       ↓
    │   [Feed-Forward Networks]
    │       ↓
    │   [Token Probability Distribution]
    │       ↓
    │   [Sampling Strategy] ← Affected by temperature, top-k, top-p
    │       ↓
    │   New Token
    │       ↓
    ├── Add to Sequence
    │
    ↓ (Repeat until complete)
Complete Token Sequence
    ↓
[EnCodec Decoder]
    ↓
Raw Audio Waveform
    ↓
[Post-Processing]
    ↓
Final Audio Output
</code></pre></div></div>

<p>This visualization helps us trace how information flows through the system and where different parameters influence the process.</p>

<h2 id="memory-management-in-audiocraft">Memory Management in AudioCraft</h2>

<p>Understanding the architecture also helps explain memory usage, which is crucial for running these models efficiently:</p>

<h3 id="model-size-and-memory-footprint">Model Size and Memory Footprint</h3>

<p>Each model size has different memory requirements:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Approximate memory usage
</span><span class="n">memory_usage</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'small'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'model_parameters'</span><span class="p">:</span> <span class="s">'300MB'</span><span class="p">,</span>
        <span class="s">'runtime_cuda'</span><span class="p">:</span> <span class="s">'2-3GB'</span><span class="p">,</span>
        <span class="s">'runtime_mps'</span><span class="p">:</span> <span class="s">'2-3GB'</span><span class="p">,</span>
        <span class="s">'runtime_cpu'</span><span class="p">:</span> <span class="s">'4GB'</span>
    <span class="p">},</span>
    <span class="s">'medium'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'model_parameters'</span><span class="p">:</span> <span class="s">'1.5GB'</span><span class="p">,</span>
        <span class="s">'runtime_cuda'</span><span class="p">:</span> <span class="s">'4-6GB'</span><span class="p">,</span>
        <span class="s">'runtime_mps'</span><span class="p">:</span> <span class="s">'4-6GB'</span><span class="p">,</span>
        <span class="s">'runtime_cpu'</span><span class="p">:</span> <span class="s">'8GB'</span>
    <span class="p">},</span>
    <span class="s">'large'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s">'model_parameters'</span><span class="p">:</span> <span class="s">'3GB'</span><span class="p">,</span>
        <span class="s">'runtime_cuda'</span><span class="p">:</span> <span class="s">'8-12GB'</span><span class="p">,</span>
        <span class="s">'runtime_mps'</span><span class="p">:</span> <span class="s">'8-12GB'</span><span class="p">,</span>
        <span class="s">'runtime_cpu'</span><span class="p">:</span> <span class="s">'16GB'</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The memory footprint comes from:</p>
<ol>
  <li><strong>Model Parameters</strong>: Weights in the transformer layers</li>
  <li><strong>Attention Caches</strong>: Memory used to store previous token representations</li>
  <li><strong>Activation Maps</strong>: Intermediate values during computation</li>
  <li><strong>Input/Output Buffers</strong>: Memory for processing inputs and outputs</li>
</ol>

<h3 id="where-memory-is-used-in-the-architecture">Where Memory Is Used in the Architecture</h3>

<p>Different components have different memory needs:</p>

<ul>
  <li><strong>Text Encoder</strong>: Relatively small memory footprint</li>
  <li><strong>Generator Model</strong>: The largest memory consumer, especially during attention computation</li>
  <li><strong>EnCodec Decoder</strong>: Moderate memory usage for reconstructing audio</li>
</ul>

<p>Understanding these memory patterns helps you manage resources effectively:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Memory optimization example
</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">gc</span>

<span class="c1"># Clear GPU cache between generations
</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">empty_cache</span><span class="p">()</span>
<span class="n">gc</span><span class="p">.</span><span class="n">collect</span><span class="p">()</span>

<span class="c1"># Use smaller models when possible
</span><span class="n">model</span> <span class="o">=</span> <span class="n">MusicGen</span><span class="p">.</span><span class="n">get_pretrained</span><span class="p">(</span><span class="s">'small'</span><span class="p">)</span>

<span class="c1"># Generate shorter segments for memory-constrained environments
</span><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>  <span class="c1"># Instead of longer durations
</span></code></pre></div></div>

<h2 id="architectural-insights-for-troubleshooting">Architectural Insights for Troubleshooting</h2>

<p>Understanding the architecture gives you powerful troubleshooting tools:</p>

<h3 id="problem-generation-gets-stuck-in-repetitive-patterns">Problem: Generation Gets “Stuck” in Repetitive Patterns</h3>

<p><strong>Architectural Explanation</strong>: The model is caught in a probability loop where it keeps predicting the same tokens.</p>

<p><strong>Solution</strong>: Adjust sampling parameters to encourage exploration.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Increase temperature and use nucleus sampling
</span><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>  <span class="c1"># Higher temperature
</span>    <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>          <span class="c1"># Disable top-k
</span>    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span>        <span class="c1"># Enable nucleus sampling
</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="problem-generation-doesnt-follow-the-prompt-closely">Problem: Generation Doesn’t Follow the Prompt Closely</h3>

<p><strong>Architectural Explanation</strong>: The text condition isn’t influencing generation strongly enough.</p>

<p><strong>Solution</strong>: Increase classifier-free guidance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Increase cfg_coef for stronger prompt adherence
</span><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span>
    <span class="n">cfg_coef</span><span class="o">=</span><span class="mf">5.0</span>  <span class="c1"># Default is around 3.0
</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="problem-out-of-memory-errors">Problem: Out of Memory Errors</h3>

<p><strong>Architectural Explanation</strong>: Attention computation scales quadratically with sequence length.</p>

<p><strong>Solution</strong>: Generate shorter segments or use a smaller model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reduce memory usage
</span><span class="n">model</span> <span class="o">=</span> <span class="n">MusicGen</span><span class="p">.</span><span class="n">get_pretrained</span><span class="p">(</span><span class="s">'small'</span><span class="p">)</span>  <span class="c1"># Use smaller model
</span><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>  <span class="c1"># Generate shorter audio
</span></code></pre></div></div>

<h2 id="advanced-architecture-concepts">Advanced Architecture Concepts</h2>

<p>For those interested in deeper understanding, here are some advanced concepts:</p>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p>The transformer models in AudioCraft use multi-head attention, which:</p>

<ol>
  <li>Allows the model to jointly attend to information from different representation subspaces</li>
  <li>Enables the model to capture different types of relationships at once</li>
  <li>Helps the model understand complex audio patterns</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified multi-head attention pseudocode
</span><span class="k">def</span> <span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
    <span class="c1"># Split inputs into multiple heads
</span>    <span class="n">split_q</span> <span class="o">=</span> <span class="n">split_into_heads</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">split_k</span> <span class="o">=</span> <span class="n">split_into_heads</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">split_v</span> <span class="o">=</span> <span class="n">split_into_heads</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    
    <span class="c1"># Apply attention separately to each head
</span>    <span class="n">head_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
        <span class="n">head_output</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">split_q</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">split_k</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">split_v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">head_outputs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">head_output</span><span class="p">)</span>
    
    <span class="c1"># Concatenate and project outputs
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">(</span><span class="n">head_outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">linear_projection</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="masked-generation">Masked Generation</h3>

<p>AudioCraft models use masking during generation to ensure they only condition on previous tokens:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified masked generation pseudocode
</span><span class="k">def</span> <span class="nf">masked_generation</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">text_condition</span><span class="p">):</span>
    <span class="c1"># Create causal mask (can't see future tokens)
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">create_causal_mask</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">))</span>
    
    <span class="c1"># Apply transformer with mask
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">text_condition</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    
    <span class="c1"># Get prediction for next token (last position only)
</span>    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">next_token_logits</span>
</code></pre></div></div>

<p>This ensures the generation process follows causal constraints.</p>

<h2 id="implementation-case-study-understanding-a-full-generation">Implementation Case Study: Understanding a Full Generation</h2>

<p>Let’s examine a complete generation example to see the architecture in action:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">audiocraft.models</span> <span class="kn">import</span> <span class="n">MusicGen</span>

<span class="c1"># 1. Load the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">MusicGen</span><span class="p">.</span><span class="n">get_pretrained</span><span class="p">(</span><span class="s">'small'</span><span class="p">)</span>

<span class="c1"># 2. Set generation parameters
</span><span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span>
    <span class="n">duration</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>       <span class="c1"># 10 seconds of audio
</span>    <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>     <span class="c1"># Balanced randomness
</span>    <span class="n">top_k</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>           <span class="c1"># Consider top 250 tokens
</span>    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>           <span class="c1"># Disable nucleus sampling
</span>    <span class="n">cfg_coef</span><span class="o">=</span><span class="mf">3.0</span>         <span class="c1"># Moderate text adherence
</span><span class="p">)</span>

<span class="c1"># 3. Prepare the prompt
</span><span class="n">prompt</span> <span class="o">=</span> <span class="s">"An upbeat electronic track with a catchy melody"</span>

<span class="c1"># 4. Generate audio
# Internally, this performs:
# - Text encoding
# - Token generation
# - EnCodec decoding
</span><span class="n">wav</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">([</span><span class="n">prompt</span><span class="p">])</span>

<span class="c1"># 5. Process the output
# - Move from GPU to CPU
# - Convert to appropriate format
# - Apply any post-processing
</span><span class="n">audio_cpu</span> <span class="o">=</span> <span class="n">wav</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">cpu</span><span class="p">()</span>
</code></pre></div></div>

<p>During step 4, the architecture we’ve explored comes to life:</p>

<ol>
  <li>The text encoder processes “An upbeat electronic track with a catchy melody”</li>
  <li>The generator model begins with this condition and autoregressively produces audio tokens</li>
  <li>The EnCodec decoder converts these tokens into the final waveform</li>
</ol>

<p>Each of these steps involves the complex architectural components we’ve discussed.</p>

<h2 id="extending-your-understanding">Extending Your Understanding</h2>

<p>Now that you understand AudioCraft’s architecture, you can:</p>

<h3 id="1-better-control-generation">1. Better Control Generation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fine-tuned control based on architectural understanding
</span><span class="k">def</span> <span class="nf">generate_with_control</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">style_weight</span><span class="p">,</span> <span class="n">creativity</span><span class="p">,</span> <span class="n">prompt_adherence</span><span class="p">):</span>
    <span class="s">"""Generate audio with precise control over generation qualities."""</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MusicGen</span><span class="p">.</span><span class="n">get_pretrained</span><span class="p">(</span><span class="s">'medium'</span><span class="p">)</span>
    
    <span class="c1"># Map high-level controls to architectural parameters
</span>    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">creativity</span> <span class="o">*</span> <span class="mf">1.5</span>  <span class="c1"># 0.5 to 2.0
</span>    <span class="n">top_k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">50</span> <span class="o">+</span> <span class="n">creativity</span> <span class="o">*</span> <span class="mi">200</span><span class="p">)</span>    <span class="c1"># 50 to 250
</span>    <span class="n">cfg_coef</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">prompt_adherence</span> <span class="o">*</span> <span class="mf">7.0</span>  <span class="c1"># 1.0 to 8.0
</span>    
    <span class="c1"># Apply style weight through classifier-free guidance
</span>    <span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span>
        <span class="n">duration</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">cfg_coef</span><span class="o">=</span><span class="n">cfg_coef</span>
    <span class="p">)</span>
    
    <span class="c1"># Generate with controlled parameters
</span>    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">([</span><span class="n">prompt</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="2-optimize-for-your-hardware">2. Optimize for Your Hardware</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hardware-aware generation based on architectural understanding
</span><span class="k">def</span> <span class="nf">optimize_for_hardware</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">available_memory_gb</span><span class="p">):</span>
    <span class="s">"""Select optimal model size and parameters based on available memory."""</span>
    <span class="c1"># Choose model size based on memory
</span>    <span class="k">if</span> <span class="n">available_memory_gb</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">model_size</span> <span class="o">=</span> <span class="s">'large'</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="mf">20.0</span>
    <span class="k">elif</span> <span class="n">available_memory_gb</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">model_size</span> <span class="o">=</span> <span class="s">'medium'</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="mf">15.0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model_size</span> <span class="o">=</span> <span class="s">'small'</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="mf">10.0</span>
    
    <span class="c1"># Load appropriate model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">MusicGen</span><span class="p">.</span><span class="n">get_pretrained</span><span class="p">(</span><span class="n">model_size</span><span class="p">)</span>
    
    <span class="c1"># Set parameters based on available resources
</span>    <span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="n">duration</span><span class="p">)</span>
    
    <span class="c1"># Generate with optimized settings
</span>    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">([</span><span class="n">prompt</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="3-implement-chunked-generation-for-longer-outputs">3. Implement Chunked Generation for Longer Outputs</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Chunked generation leveraging architectural knowledge
</span><span class="k">def</span> <span class="nf">generate_long_audio</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">total_duration</span><span class="o">=</span><span class="mf">60.0</span><span class="p">,</span> <span class="n">chunk_duration</span><span class="o">=</span><span class="mf">10.0</span><span class="p">):</span>
    <span class="s">"""Generate longer audio by chunking with architectural awareness."""</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MusicGen</span><span class="p">.</span><span class="n">get_pretrained</span><span class="p">(</span><span class="s">'medium'</span><span class="p">)</span>
    
    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current_duration</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">while</span> <span class="n">current_duration</span> <span class="o">&lt;</span> <span class="n">total_duration</span><span class="p">:</span>
        <span class="c1"># Adjust prompt for continuation
</span>        <span class="k">if</span> <span class="n">current_duration</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">chunk_prompt</span> <span class="o">=</span> <span class="n">prompt</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chunk_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"Continue the </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s"> seamlessly"</span>
        
        <span class="c1"># Generate chunk
</span>        <span class="n">model</span><span class="p">.</span><span class="n">set_generation_params</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="n">chunk_duration</span><span class="p">)</span>
        <span class="n">wav</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">([</span><span class="n">chunk_prompt</span><span class="p">])</span>
        
        <span class="c1"># Store chunk
</span>        <span class="n">chunks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">wav</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">current_duration</span> <span class="o">+=</span> <span class="n">chunk_duration</span>
    
    <span class="c1"># Combine chunks (simple concatenation - a real implementation would use crossfading)
</span>    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
    <span class="n">combined</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="hands-on-challenge">Hands-on Challenge</h2>

<p>Now it’s your turn to apply your architectural knowledge with this challenge:</p>

<h3 id="challenge-parameter-space-explorer">Challenge: Parameter Space Explorer</h3>

<p>Create a script that systematically explores how different architectural parameters affect generation. The script should:</p>

<ol>
  <li>Generate audio samples using a grid of parameter combinations</li>
  <li>Save each sample with metadata describing the parameters used</li>
  <li>Create a simple HTML report showing the relationship between parameters and audio qualities</li>
  <li>Identify optimal parameter combinations for different types of content</li>
</ol>

<p>This challenge will reinforce your understanding of how the architecture responds to different parameter settings.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ul>
  <li>AudioCraft consists of three main components: text encoder, audio generator, and EnCodec decoder</li>
  <li>The models use transformer architectures with autoregressive generation</li>
  <li>MusicGen and AudioGen share core architecture but are specialized for different audio types</li>
  <li>Generation parameters directly influence how the architecture behaves</li>
  <li>Understanding the architecture enables better troubleshooting and optimization</li>
  <li>Memory usage is related to model size, sequence length, and architectural complexity</li>
</ul>

<h2 id="next-steps">Next Steps</h2>

<p>Now that you understand AudioCraft’s architecture, you’re ready to explore:</p>

<ul>
  <li><a href="/chapters/part1/first-generation/">Your First Audio Generation</a>: Apply your architectural understanding in practical examples</li>
  <li><a href="/chapters/part2/basic-music/">Basic Music Generation</a>: Learn to generate music using MusicGen</li>
  <li><a href="/chapters/part3/basic-sound-effects/">Basic Sound Effect Generation</a>: Create environmental sounds with AudioGen</li>
</ul>

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2306.05284">MusicGen Research Paper</a>: Simple and Controllable Music Generation</li>
  <li><a href="https://arxiv.org/abs/2209.15352">AudioGen Research Paper</a>: Textually Guided Audio Generation</li>
  <li><a href="https://arxiv.org/abs/2210.13438">EnCodec Research Paper</a>: High Fidelity Neural Audio Compression</li>
  <li><a href="https://arxiv.org/abs/1706.03762">Transformer Architecture Explained</a>: Attention Is All You Need - The original transformer paper</li>
</ul>

  
  
  
  
  
  
  <div class="next-steps">
    <h2>Next Steps</h2>
    <p>Now that you've mastered 3 understanding audiocraft architecture, you're ready to explore:</p>
    <ul>
      
      <li>
        <a href="/chapters/part1/first-generation/">Your First Audio Generation</a>: Apply your architectural understanding in practical examples
      </li>
      
      <li>
        <a href="/chapters/part2/basic-music/">Basic Music Generation</a>: Learn to generate music using MusicGen
      </li>
      
      <li>
        <a href="/chapters/part3/basic-sound-effects/">Basic Sound Effect Generation</a>: Create environmental sounds with AudioGen
      </li>
      
    </ul>
  </div>
  
  
  
  <div class="further-reading">
    <h2>Further Reading</h2>
    <ul>
      
      <li>
        
        <a href="https://arxiv.org/abs/2306.05284" target="_blank">MusicGen Research Paper</a>
        
        
        - Simple and Controllable Music Generation
        
      </li>
      
      <li>
        
        <a href="https://arxiv.org/abs/2209.15352" target="_blank">AudioGen Research Paper</a>
        
        
        - Textually Guided Audio Generation
        
      </li>
      
      <li>
        
        <a href="https://arxiv.org/abs/2210.13438" target="_blank">EnCodec Research Paper</a>
        
        
        - High Fidelity Neural Audio Compression
        
      </li>
      
      <li>
        
        <a href="https://arxiv.org/abs/1706.03762" target="_blank">Transformer Architecture Explained</a>
        
        
        - Attention Is All You Need - The original transformer paper
        
      </li>
      
    </ul>
  </div>
  
</div>

<div class="chapter-navigation">
  
  
  
  <a href="/chapters/part1/first-generation/" class="next-chapter">
    <span class="nav-label">Next</span>
    <span class="nav-title">Chapter 4: Your First Audio Generation</span>
  </a>
  
</div>

<!-- Copyright footer for all chapter pages -->
<div class="copyright-footer">
  <hr>
  <p>
    Copyright © 2025 Scott Friedman.
    <br>
    This work is licensed under the <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.
  </p>
</div>


        </div>
      </main>

      <footer class="site-footer">
        <div class="container">
          <div class="footer-col-wrapper">
            <div class="footer-col">
              <p>A hands-on guide to creating music, sound effects, and audio experiences with AI</p>
              <p>
                <a href="https://github.com/facebookresearch/audiocraft">AudioCraft GitHub Repository</a>
              </p>
            </div>
            <div class="footer-col">
              <p class="copyright">
                Copyright © 2025 Scott Friedman.<br>
                Licensed under <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a>.
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </body>
</html>